<p align="center">
    <img src="assets/logo.png" width="150" style="margin-bottom: 0.2;"/>
<p>

<h5 align="center"> üå∏ "Just when the caterpillar thought the world was over, it became a butterfly." ü¶ã </h2>

<h3 align="center"><a href="https://arxiv.org/pdf/2411.08147">
Large Language Models Can Self-Improve in Long-context Reasoning</a></h3>
<!-- <h5 align="center"> If our project helps you, please give us a star ‚≠ê on GitHub to support us. üôèüôè </h2> -->

<h5 align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2411.08147-AD1C18.svg?logo=arXiv)](https://arxiv.org/pdf/2411.08147)
[![hf_paper](https://img.shields.io/badge/%F0%9F%A4%97-Paper-FF6F61
)](https://huggingface.co/papers/2411.08147)
[![hf_model_data](https://img.shields.io/badge/%F0%9F%A4%97-Models&Datasets-48A9DC
)](https://huggingface.co/collections/Siheng99/sealong-67313e3b4edd034cb4a76cc5)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

</h5>

## üì∞ News
* **[2024.11.10]**  Release training and evaluation codes, models, and datasets for SEALONG.


## üõ†Ô∏è Requirements and Installation
**Basic Dependencies**:
* Python >= 3.10
* Pytorch >= 2.4.0
* CUDA Version >= 12.1

**Install required packages**:
```bash
git clone https://github.com/SihengLi99/SEALONG
pip install -r requirements.txt
```

## üîë Usage
**Model Usage**:
```python
import transformers
import torch

model_id = "Siheng99/Llama-3.1-8B-Instruct-SEALONG"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "Who are you?"},
]

outputs = pipeline(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])
```

**Data Usage**:
```python
from datasets import load_dataset
dataset = load_dataset("Siheng99/Llama-3.1-8B-Instruct-SEALONG-Dataset")
print(dataset)
print(dataset["train"][0])
```


## üìä Evaluation
```bash
bash scripts/eval_longbench_qa.sh
```
Note: Set MODEL_NAME_OR_PATH to the desired target model.

## üî• Training
### Data Preparation

#### 1. Synthesizing Your Own Data
**Download MuSiQue**:
```bash
cd data
gdown 'https://drive.google.com/uc?export=download&id=1tGdADlNjWFaHLeZZGShh2IRcpO6Lv24h'
unzip musique_data_v1.0.zip -d musique && mv musique/data/* musique/ 
rm -r musique/data && rm musique_data_v1.0.zip
```

**Process MuSiQue**:
```bash
bash scripts/process_data.sh
```

**Synthesize Training Data**:
```bash
bash scripts/synthesize.sh
```

#### 2. Using Our Pre-synthesized Data
```python
from datasets import load_dataset
dataset = load_dataset("Siheng99/Llama-3.1-8B-Instruct-SEALONG-Dataset")
dataset.save_to_disk(/path/to/your/save_dir)
```

### Fine-tuning

Set MODEL_NAME_OR_PATH and DATASET in the scripts before fine-tuning.

**ORPO**:
```bash
# QLoRA
bash scripts/finetune_orpo_qlora_xtuner.sh
# Full-parameter
bash scripts/finetune_orpo_xtuner.sh
```

**SFT**:

You may also opt for SFT; however, our findings indicate that ORPO achieves superior performance (see Table 5 in our paper).
```bash
# QLoRA
bash scripts/finetune_sft_qlora_xtuner.sh
# Full-parameter
bash scripts/finetune_sft_xtuner.sh
```

In our experiments, we select QLoRA for memory efficiency, but we also test full parameter training. We observe that a learning rate of 5e-6 yields decent performance when using ORPO with full parameter training.

## üìë Citation

If SEALONG is useful for your research or applications, please cite it with the following BibTeX:
```bibtex
@article{li2024large,
  title={Large Language Models Can Self-Improve in Long-context Reasoning},
  author={Li, Siheng and Yang, Cheng and Cheng, Zesen and Liu, Lemao and Yu, Mo and Yang, Yujiu and Lam, Wai},
  journal={arXiv preprint arXiv:2411.08147},
  year={2024}
}
```

## üëç Acknowledgement
We gratefully acknowledge the following projects that SEALONG builds upon:
* [**MuSiQue**](https://github.com/StonyBrookNLP/musique)
* [**XTuner**](https://github.com/InternLM/xtuner)
* [**DeepSpeed**](https://github.com/microsoft/DeepSpeed)
* [**transformers**](https://github.com/huggingface/transformers)
* [**datasets**](https://github.com/huggingface/datasets)
* [**peft**](https://github.com/huggingface/peft)
* [**orpo**](https://github.com/xfactlab/orpo)
